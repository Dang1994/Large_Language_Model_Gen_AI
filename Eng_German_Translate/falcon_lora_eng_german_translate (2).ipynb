{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5efOnpwtaEYQ"
      },
      "source": [
        "\n",
        "## **English-to-German Machine Translation using Falcon-LoRA**\n",
        "\n",
        "**Task**: Text-to-text\n",
        "\n",
        "In this project, I fine-tuned **Falcon-1B** (developed by TII, Dubai) with **LoRA** (Low-Rank Adaptation) to build an efficient **English-to-German translation system**. The implementation leverages **parameter-efficient fine-tuning** to achieve high-quality translations while minimizing GPU resource requirements, making it suitable for environments with limited computational resources.\n",
        "\n",
        "Data Source: [English-German](https://www.kaggle.com/datasets/kaushal2896/english-to-german)\n",
        "## Key Steps Overview:\n",
        "\n",
        "1. **Data Preparation**: Curate a parallel English-German corpus.\n",
        "2. **Tokenizer and Model Loading**: Load Falcon-1B model and tokenizer from Hugging Face, incorporating **1-bit quantization** for efficient memory usage.\n",
        "3. **LoRA Configuration**: Apply **LoRA** (Low-Rank Adaptation) to inject trainable adapters into Falconâ€™s attention layers.\n",
        "4. **Training**: Fine-tune the model on a single GPU (e.g., NVIDIA T4 or A10G) using Hugging Face's Transformers library.\n",
        "5. **Evaluation**: Model performance evaluation using **BLEU score**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa8E37bZwHNL"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "62hMZ134ZInn"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "import gradio as gr\n",
        "from translation import Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOHOFbzdHJ7"
      },
      "source": [
        " **Tokenizer and Model Loading**:\n",
        "   - The **Falcon-1B model** and **tokenizer** are loaded from the Hugging Face model hub.\n",
        "   - The model is loaded with **1-bit quantization** to reduce memory usage, enabling efficient use of GPUs with limited memory (e.g., NVIDIA T4/A10G)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "98Z6fA4-Z5pu"
      },
      "outputs": [],
      "source": [
        "# Quantization configuration for reduced memory usage (helpful for Colab)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Enable 8-bit quantization\n",
        "    llm_int8_threshold=6.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h91NSbJOZ-TS"
      },
      "outputs": [],
      "source": [
        "# Configure model and tokenizer\n",
        "model_name = \"tiiuae/falcon-rw-1b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aFHTost_mSP",
        "outputId": "1598e2ed-7193-4350-a064-5421e23fabdb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfqpafjaZ3BJ"
      },
      "outputs": [],
      "source": [
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",  # Use automatic device placement (e.g., GPU if available)\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra9zO377cTY8"
      },
      "source": [
        "**Data Preparation**:\n",
        "- I used a curated parallel **English-German corpus** for training the translation model. This dataset consists of pairs of English sentences and their German translations, formatted into a tab-separated text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkClje0Z0qN"
      },
      "outputs": [],
      "source": [
        "# Prepare and split dataset\n",
        "def load_and_prepare_data(file_path):\n",
        "    raw_lines = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                raw_lines.append({'input': parts[0], 'output': parts[1]})\n",
        "    return Dataset.from_list(raw_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coOazS95Zx_x"
      },
      "outputs": [],
      "source": [
        "# Load and split data (80% train, 20% validation)\n",
        "dataset = load_and_prepare_data('deu.txt').train_test_split(test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yx-4b_LZs7c"
      },
      "outputs": [],
      "source": [
        "# Formatting function\n",
        "def format_dataset(example):\n",
        "    return {\n",
        "        \"text\": f\"Translate English to German:\\nEnglish: {example['input']}\\nGerman:\",\n",
        "        \"labels\": example['output']\n",
        "    }\n",
        "\n",
        "train_dataset = dataset['train'].map(format_dataset)\n",
        "val_dataset = dataset['test'].map(format_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtxRq_fJZnTD"
      },
      "outputs": [],
      "source": [
        "# Tokenization with proper padding/truncation\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize labels separately\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"labels\"],\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0JKkWw6Zi_P"
      },
      "outputs": [],
      "source": [
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIZZG2gUZfoq"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns\n",
        "tokenized_train = tokenized_train.remove_columns([\"text\", \"input\", \"output\"])\n",
        "tokenized_val = tokenized_val.remove_columns([\"text\", \"input\", \"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxzER98FdP86"
      },
      "source": [
        "**LoRA Configuration**:\n",
        "   - To make the model more parameter-efficient, I applied **LoRA** (Low-Rank Adaptation). LoRA injects trainable low-rank adapters into the attention layers, allowing for fine-tuning with fewer parameters and reduced memory consumption.\n",
        "   - This method reduces computational cost while maintaining high performance for the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbfWxwLvZbdz"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"query_key_value\"]  # Specific to Falcon architecture\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYaNtj3eZTLC"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"lora-falcon-output\",\n",
        "    #num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"adamw_torch\",\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_dir=\"logs\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    warmup_steps=100,\n",
        "    dataloader_num_workers=2  # To avoid issues related to dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yga56gqYZPGr"
      },
      "outputs": [],
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv1Z9eSWZL65"
      },
      "outputs": [],
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gh2LoQj8z2F"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts35ZwjJYwMC"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"lora-falcon-finetuned\")\n",
        "tokenizer.save_pretrained(\"lora-falcon-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGs6IpfjeCaM"
      },
      "source": [
        "**Inference**:\n",
        "   - Finally, I used the trained model to perform **inference** on a few sample sentences. The model generates the German translation for an English input, which can be evaluated by comparing it to the ground truth translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW7zBY3GY5U8"
      },
      "outputs": [],
      "source": [
        "# Inference function\n",
        "def generate_translation(model, tokenizer, english_text):\n",
        "    prompt = f\"Translate English to German:\\nEnglish: {english_text}\\nGerman:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    # Decode only the generated German text\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    german_translation = full_output.split(\"German:\")[1].strip()\n",
        "    return german_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4gnB5O9Y_pl"
      },
      "outputs": [],
      "source": [
        "# Test translation\n",
        "test_text = \"Where is the train station?\"\n",
        "translation = generate_translation(model, tokenizer, test_text)\n",
        "print(f\"\\nTest Translation:\\nEnglish: {test_text}\\nGerman: {translation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWBJimRfpWcR"
      },
      "outputs": [],
      "source": [
        "# Initialize the translator\n",
        "translator = Translator(model_path=\"lora-falcon-finetuned\")\n",
        "\n",
        "# Define the translation function\n",
        "def translate_text(text):\n",
        "    # Generate German translation\n",
        "    translation = translator(text)\n",
        "    return translation\n",
        "\n",
        "# Create the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=translate_text,\n",
        "    inputs=gr.Textbox(label=\"Enter text in English\", placeholder=\"Type your text here...\"),\n",
        "    outputs=gr.Textbox(label=\"German Translation\"),\n",
        "    title=\"English-to-German Translator by Subrat Kumar\",\n",
        "    description=\"This app translates English sentences to German using a fine-tuned Falcon-1B model with LoRA.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
